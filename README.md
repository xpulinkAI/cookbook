# XPULink API Cookbook

**Build powerful AI applications with zero infrastructure hassle** - A comprehensive collection of examples for [www.xpulink.net](https://www.xpulink.net) ğŸš€

## Why XPULink?

### ğŸ¯ **No GPU? No Problem!**
- **100% Cloud-Hosted**: All models run on XPULink's infrastructure
- **Zero Setup**: No CUDA, no drivers, no expensive hardware needed
- **Instant Access**: Get started in minutes with just an API key

### âš¡ **Powered by vLLM - Enterprise-Grade Performance**
- **15-30x Faster** than traditional inference frameworks
- **50% Better Memory Efficiency** with PagedAttention technology
- **High Concurrency**: Handle thousands of requests simultaneously
- **Low Latency**: Optimized CUDA kernels for blazing-fast responses

### ğŸ”Œ **OpenAI-Compatible API**
- Drop-in replacement for OpenAI API
- Use with LangChain, LlamaIndex, and other popular frameworks
- Minimal code changes to switch from OpenAI

### ğŸ’° **Cost-Effective**
- Pay only for what you use
- No idle infrastructure costs
- Transparent pricing

---

## ğŸ“š What's Inside

This cookbook provides production-ready examples for:

| Feature | Description | Best For |
|---------|-------------|----------|
| ğŸ¤– **Text Generation** | Basic LLM inference with Qwen3-32B | Chat, content generation |
| ğŸ“„ **RAG System** | PDF Q&A with BGE-M3 embeddings | Document analysis, knowledge bases |
| ğŸ¯ **LoRA Fine-tuning** | Custom model training | Domain adaptation, style transfer |
| ğŸ­ **Device Monitoring Agent** | Industrial IoT diagnostics | Predictive maintenance, anomaly detection |
| ğŸ“Š **Model Evaluation** | Benchmark testing with OpenBench | Model comparison, performance analysis |

**All examples now use LiteLLM** for elegant, production-ready integration with custom APIs!

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- XPULink API Key from [www.xpulink.net](https://www.xpulink.net)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd cookbook

# Install dependencies
pip install -r requirements.txt

# Set up your API key
echo "XPULINK_API_KEY=your_api_key_here" > .env
```

### Your First API Call (30 seconds!)

```python
from litellm import completion

response = completion(
    model="openai/qwen3-32b",
    messages=[{"role": "user", "content": "Hello!"}],
    api_key="your_api_key",
    api_base="https://www.xpulink.net/v1",
    custom_llm_provider="openai"
)

print(response.choices[0].message.content)
```

**That's it! No GPU setup, no model downloads, just pure API magic.** âœ¨

---

## ğŸ“– Examples

### 1. ğŸ’¬ Text Generation

**The simplest way to use LLMs**

```bash
cd function_call
python text_model.py
```

**What you get:**
- OpenAI-compatible chat completions
- Streaming support
- Function calling (when available)
- Full control over temperature, tokens, etc.

**Why it's easy with XPULink:**
- âœ… No model downloads (GBs of data)
- âœ… No GPU required
- âœ… Instant API access
- âœ… Auto-scaling infrastructure

---

### 2. ğŸ“„ RAG System (Retrieval-Augmented Generation)

**Build ChatGPT for your documents**

```bash
cd RAG

# Put your PDFs in data/
mkdir -p data
cp your_document.pdf data/

# Run the system
python pdf_rag_bge_m3.py
```

**Features:**
- ğŸŒ **BGE-M3 Embeddings**: Best-in-class multilingual model
- ğŸ“š **PDF Processing**: Automatic text extraction and chunking
- ğŸ” **Semantic Search**: Find relevant context for any question
- ğŸ¤– **LLM Integration**: Generate answers based on your documents
- ğŸ’¾ **Vector Storage**: Efficient retrieval with LlamaIndex

**Why RAG on XPULink:**
- âœ… **No Embedding Server**: BGE-M3 hosted for you
- âœ… **No LLM Hosting**: Qwen3-32B ready to use
- âœ… **Automatic Retries**: Built-in error handling
- âœ… **LiteLLM Integration**: Clean, maintainable code

**Use Cases:**
- Corporate knowledge bases
- Customer support bots
- Research paper analysis
- Legal document search

See `RAG/README.md` for detailed documentation.

---

### 3. ğŸ¯ LoRA Fine-tuning

**Customize models for your specific needs - on the cloud!**

```bash
cd LoRA

# Interactive notebook (recommended)
jupyter notebook lora_finetune_example.ipynb

# Or use Python script
python lora_finetune.py
```

**What is LoRA?**
- **Parameter-Efficient**: Train only 0.1% of model parameters
- **Fast**: Minutes to hours (vs. days for full fine-tuning)
- **Cheap**: Much lower compute costs
- **Effective**: Near full fine-tuning quality

**Why Fine-tune on XPULink:**
- âœ… **Cloud Training**: Zero local GPU needed
- âœ… **Managed Infrastructure**: We handle everything
- âœ… **Easy API**: Upload, configure, train, deploy
- âœ… **Quick Turnaround**: Get results fast

**Perfect For:**
- ğŸ¢ **Enterprise**: Inject company knowledge
- ğŸ¥ **Domain Experts**: Medical, legal, finance terminology
- âœï¸ **Style**: Custom tone, format, personality
- ğŸ¯ **Task Optimization**: Code generation, summarization, etc.

**Example:**
```python
from lora_finetune import XPULinkLoRAFineTuner

finetuner = XPULinkLoRAFineTuner()

# Prepare data
training_data = [
    {
        "messages": [
            {"role": "system", "content": "You are a Python expert."},
            {"role": "user", "content": "Explain decorators"},
            {"role": "assistant", "content": "Decorators in Python..."}
        ]
    },
    # ... more examples
]

# Train in the cloud
file_id = finetuner.upload_training_file("training.jsonl")
job_id = finetuner.create_finetune_job(file_id, model="qwen3-32b")
status = finetuner.wait_for_completion(job_id)

# Use your custom model
finetuned_model = status['fine_tuned_model']
```

See `LoRA/README.md` for best practices and advanced configuration.

---

### 4. ğŸ­ Device Monitoring Agent

**AI-powered predictive maintenance**

```bash
cd Agent

# Interactive demo
jupyter notebook device_agent_example.ipynb

# Or quick test
python simple_example.py
```

**Capabilities:**
- ğŸ“Š **Real-time Analysis**: Multi-sensor data interpretation
- ğŸ“ **Log Intelligence**: Pattern recognition in error logs
- ğŸ”§ **Maintenance Planning**: Predictive scheduling
- ğŸ“ˆ **Trend Analysis**: Identify degradation patterns
- ğŸ“‹ **Automated Reports**: Structured diagnostic output

**Industry Applications:**
- Manufacturing: Production line monitoring
- Energy: Power generation equipment
- Transportation: Fleet management
- Data Centers: Server health monitoring

**Why on XPULink:**
- âœ… **Always Available**: 24/7 cloud inference
- âœ… **No Latency Issues**: Fast response times
- âœ… **Scalable**: Monitor thousands of devices
- âœ… **Cost-Effective**: No dedicated servers needed

See `Agent/README.md` for implementation details.

---

### 5. ğŸ“Š Model Evaluation

**Benchmark your models with OpenBench**

```bash
cd Evaluation

# Install OpenBench
pip install openbench

# Run evaluation
openbench evaluate \
  --model-type openai \
  --model-name qwen3-32b \
  --api-key $XPULINK_API_KEY \
  --base-url https://www.xpulink.net/v1 \
  --benchmark mmlu
```

**Supported Benchmarks:**
- MMLU (Massive Multitask Language Understanding)
- GSM8K (Math reasoning)
- HellaSwag (Common sense reasoning)
- Custom benchmarks

See `Evaluation/README.md` for comprehensive guide.

---

## ğŸ—ï¸ Architecture

### Built on vLLM - The Fastest Inference Engine

XPULink uses **vLLM** (Very Large Language Model) for all model serving:

| Feature | vLLM (XPULink) | Traditional Frameworks |
|---------|---------------|----------------------|
| **Throughput** | âš¡ **15-30x faster** | 1x baseline |
| **Memory** | ğŸ’¾ **50% more efficient** | Standard |
| **Latency** | ğŸš€ **Dynamic batching** | Static batching |
| **Concurrency** | ğŸŒ **Thousands of users** | Limited |
| **API** | âœ… **OpenAI compatible** | Custom |

**Key Technologies:**
- **PagedAttention**: Revolutionary memory management
- **Continuous Batching**: No waiting for batch completion
- **Tensor Parallelism**: Multi-GPU scaling
- **Quantization**: FP16, INT8 support

**Learn more:** [vLLM GitHub](https://github.com/vllm-project/vllm)

---

## ğŸ› ï¸ Technical Stack

### LiteLLM Integration

All examples use **LiteLLM** for elegant API integration:

```python
from litellm import completion

# Clean, consistent API across all providers
response = completion(
    model="openai/qwen3-32b",
    messages=[...],
    api_key=api_key,
    api_base="https://www.xpulink.net/v1",
    custom_llm_provider="openai"
)
```

**Why LiteLLM:**
- âœ… **No Hacks**: No workarounds or monkey-patching
- âœ… **Production-Ready**: Used by thousands of developers
- âœ… **Unified Interface**: Works with 100+ LLM providers
- âœ… **Built-in Retries**: Automatic error handling
- âœ… **Easy Migration**: Switch providers with one line

---

## ğŸ“ Project Structure

```
cookbook/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Shared dependencies
â”‚
â”œâ”€â”€ function_call/                     # Basic text generation
â”‚   â”œâ”€â”€ text_model.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ RAG/                              # Document Q&A system
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ pdf_rag_bge_m3.py            # Main RAG system
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ data/                         # Your PDFs go here
â”‚
â”œâ”€â”€ LoRA/                             # Model fine-tuning
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ lora_finetune.py             # Fine-tuning manager
â”‚   â”œâ”€â”€ lora_finetune_example.ipynb  # Interactive tutorial
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ data/                         # Training data
â”‚
â”œâ”€â”€ Agent/                            # Device monitoring
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ device_agent.py              # Agent implementation
â”‚   â”œâ”€â”€ simple_example.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ data/                         # Sample device data
â”‚
â””â”€â”€ Evaluation/                       # Model benchmarking
    â””â”€â”€ README.md
```

---

## ğŸ’¡ Best Practices

### API Key Security
```bash
# âœ… DO: Use environment variables
XPULINK_API_KEY=your_key python script.py

# âŒ DON'T: Hardcode keys
api_key = "sk-..."  # Never do this!
```

### Error Handling
```python
# LiteLLM provides automatic retries
response = completion(
    model="openai/qwen3-32b",
    messages=[...],
    api_key=api_key,
    api_base="https://www.xpulink.net/v1",
    custom_llm_provider="openai",
    num_retries=3  # Automatic retry on failure
)
```

### Performance Optimization
- Use appropriate `temperature` for your use case
- Set reasonable `max_tokens` limits
- Batch requests when possible
- Use streaming for real-time applications

---

## ğŸ¤ Support & Community

### Getting Help
- ğŸ“š **Documentation**: [www.xpulink.net/docs](https://www.xpulink.net/docs)
- ğŸ’¬ **Issues**: Open an issue on GitHub
- ğŸ“§ **Email**: support@xpulink.net
- ğŸŒ **Website**: [www.xpulink.net](https://www.xpulink.net)

### Contributing
We welcome contributions! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

---

## ğŸ“œ License

MIT License - see LICENSE file for details

---

## ğŸŒŸ Why Developers Love XPULink

> "No GPU setup, no model downloads - I had a RAG system running in 10 minutes!"
> â€” *Sarah, ML Engineer*

> "The fine-tuning API saved us weeks of infrastructure work. Just upload and train."
> â€” *Mike, Startup Founder*

> "vLLM performance + OpenAI compatibility = perfect combo"
> â€” *Alex, DevOps Lead*

---

## ğŸš€ Ready to Build?

1. **Get your API key**: [www.xpulink.net](https://www.xpulink.net)
2. **Pick an example**: Start with RAG or text generation
3. **Run the code**: Copy, paste, customize
4. **Ship to production**: Scale with confidence

**No credit card needed to start experimenting!** ğŸ‰

---

**Built with â¤ï¸ by the XPULink team**

*Powered by vLLM | OpenAI-Compatible | Production-Ready*
